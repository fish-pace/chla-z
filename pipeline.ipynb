{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdbc84e2-b0db-4efe-8ae2-00d2555a2114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-cloud-storage\n",
      "  Downloading google_cloud_storage-3.7.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting google-auth<3.0.0,>=2.26.1 (from google-cloud-storage)\n",
      "  Downloading google_auth-2.43.0-py2.py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting google-api-core<3.0.0,>=2.27.0 (from google-cloud-storage)\n",
      "  Downloading google_api_core-2.28.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting google-cloud-core<3.0.0,>=2.4.2 (from google-cloud-storage)\n",
      "  Downloading google_cloud_core-2.5.0-py3-none-any.whl.metadata (3.1 kB)\n",
      "Collecting google-resumable-media<3.0.0,>=2.7.2 (from google-cloud-storage)\n",
      "  Downloading google_resumable_media-2.8.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.22.0 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from google-cloud-storage) (2.32.5)\n",
      "Collecting google-crc32c<2.0.0,>=1.1.3 (from google-cloud-storage)\n",
      "  Downloading google_crc32c-1.7.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core<3.0.0,>=2.27.0->google-cloud-storage)\n",
      "  Downloading googleapis_common_protos-1.72.0-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 (from google-api-core<3.0.0,>=2.27.0->google-cloud-storage)\n",
      "  Downloading protobuf-6.33.2-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Collecting proto-plus<2.0.0,>=1.22.3 (from google-api-core<3.0.0,>=2.27.0->google-cloud-storage)\n",
      "  Using cached proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from google-auth<3.0.0,>=2.26.1->google-cloud-storage) (6.2.0)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3.0.0,>=2.26.1->google-cloud-storage)\n",
      "  Using cached pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3.0.0,>=2.26.1->google-cloud-storage)\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from requests<3.0.0,>=2.22.0->google-cloud-storage) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from requests<3.0.0,>=2.22.0->google-cloud-storage) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from requests<3.0.0,>=2.22.0->google-cloud-storage) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from requests<3.0.0,>=2.22.0->google-cloud-storage) (2025.8.3)\n",
      "Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.26.1->google-cloud-storage)\n",
      "  Using cached pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Downloading google_cloud_storage-3.7.0-py3-none-any.whl (303 kB)\n",
      "Downloading google_api_core-2.28.1-py3-none-any.whl (173 kB)\n",
      "Downloading google_auth-2.43.0-py2.py3-none-any.whl (223 kB)\n",
      "Downloading google_cloud_core-2.5.0-py3-none-any.whl (29 kB)\n",
      "Downloading google_crc32c-1.7.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
      "Downloading google_resumable_media-2.8.0-py3-none-any.whl (81 kB)\n",
      "Downloading googleapis_common_protos-1.72.0-py3-none-any.whl (297 kB)\n",
      "Using cached proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading protobuf-6.33.2-cp39-abi3-manylinux2014_x86_64.whl (323 kB)\n",
      "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Using cached pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Installing collected packages: pyasn1, protobuf, google-crc32c, rsa, pyasn1-modules, proto-plus, googleapis-common-protos, google-resumable-media, google-auth, google-api-core, google-cloud-core, google-cloud-storage\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/12\u001b[0m [google-cloud-storage]gle-api-core]-media]\n",
      "\u001b[1A\u001b[2KSuccessfully installed google-api-core-2.28.1 google-auth-2.43.0 google-cloud-core-2.5.0 google-cloud-storage-3.7.0 google-crc32c-1.7.1 google-resumable-media-2.8.0 googleapis-common-protos-1.72.0 proto-plus-1.26.1 protobuf-6.33.2 pyasn1-0.6.1 pyasn1-modules-0.4.2 rsa-4.9.1\n"
     ]
    }
   ],
   "source": [
    "!pip install google-cloud-storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "585c525e-4fd4-4d4b-af9a-a263a7264298",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxarray\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxr\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mearthaccess\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcloud\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m storage\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdask_gateway\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Gateway\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdask\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Client\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Parallel daily CHLA(z) production using Dask-Gateway.\n",
    "\n",
    "- Searches PACE L3M Rrs DAY granules via earthaccess\n",
    "- For each granule/day:\n",
    "    * downloads Rrs\n",
    "    * runs BRT CHLA(z) prediction\n",
    "    * computes integrated/peak metrics\n",
    "    * writes a daily NetCDF locally\n",
    "    * uploads to GCS\n",
    "- Skips days that already exist in GCS unless FORCE_RERUN=True\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import earthaccess\n",
    "from google.cloud import storage\n",
    "from dask_gateway import Gateway\n",
    "from dask.distributed import Client\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# CONFIG\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "# Path to your saved ML bundle (zip) – adjust as needed\n",
    "BUNDLE_PATH = \"models/chla_brt_bundle.zip\"\n",
    "\n",
    "# GCS target\n",
    "BUCKET_NAME = \"nmfs_odp_nwfsc\"\n",
    "DESTINATION_PREFIX = \"CB/fish-pace-datasets/chla-z/netcdf\"\n",
    "\n",
    "# Dask-Gateway settings\n",
    "MIN_WORKERS = 4\n",
    "MAX_WORKERS = 12\n",
    "WORKER_CORES = 4\n",
    "WORKER_MEMORY = \"32GiB\"\n",
    "\n",
    "# Spatial chunking for NetCDF output\n",
    "LAT_CHUNK = 100\n",
    "LON_CHUNK = 100\n",
    "\n",
    "# Rerun control: if False, skip days that already exist in GCS\n",
    "FORCE_RERUN = False\n",
    "\n",
    "# Optional date filtering for rrs_results (None = no filter)\n",
    "START_DATE = None  # e.g. \"2024-03-01\"\n",
    "END_DATE   = None  # e.g. \"2024-04-30\"\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# Helper: load ML bundle and build CHLA profile dataset\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "# Ensure ml_utils is available\n",
    "if not os.path.exists(\"ml_utils.py\"):\n",
    "    import subprocess\n",
    "    subprocess.run(\n",
    "        [\n",
    "            \"wget\",\n",
    "            \"-q\",\n",
    "            \"https://raw.githubusercontent.com/fish-pace/2025-tutorials/main/ml_utils.py\",\n",
    "        ],\n",
    "        check=True,\n",
    "    )\n",
    "\n",
    "import ml_utils as mu  # noqa: E402\n",
    "\n",
    "# Load the bundle once on the client side; workers will receive it via pickling\n",
    "bundle = mu.load_ml_bundle(BUNDLE_PATH)\n",
    "\n",
    "\n",
    "def build_chla_profile_dataset(CHLA: xr.DataArray) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Given CHLA(time, z, lat, lon), compute derived metrics and\n",
    "    return an xr.Dataset suitable for writing to Zarr/NetCDF.\n",
    "    \"\"\"\n",
    "    # Start from CHLA's own dataset so its coords (including z_start/z_end) win\n",
    "    ds = CHLA.to_dataset(name=\"CHLA\")\n",
    "\n",
    "    # ---- Layer thickness (z dimension) ----\n",
    "    z_start = CHLA.coords.get(\"z_start\", None)\n",
    "    z_end   = CHLA.coords.get(\"z_end\", None)\n",
    "\n",
    "    if (z_start is not None) and (z_end is not None):\n",
    "        z_thick = (z_end - z_start).rename(\"z_thickness\")   # (z)\n",
    "    else:\n",
    "        # fallback: uniform layer thickness, e.g. 10 m\n",
    "        z_thick = xr.full_like(CHLA[\"z\"], 10.0).rename(\"z_thickness\")\n",
    "\n",
    "    z_center = CHLA[\"z\"]\n",
    "\n",
    "    # total CHLA in column (used for validity + center-of-mass)\n",
    "    col_total = CHLA.sum(\"z\")          # (time, lat, lon)\n",
    "    valid = col_total > 0              # True where there is some CHLA\n",
    "\n",
    "    # ---- Integrated CHLA (nominal 0–200 m; actual range = z extent) ----\n",
    "    CHLA_int = (CHLA * z_thick).sum(\"z\")\n",
    "    CHLA_int = CHLA_int.where(valid)\n",
    "    CHLA_int.name = \"CHLA_int_0_200\"\n",
    "\n",
    "    # ---- Peak value and depth (NaN-safe) ----\n",
    "    CHLA_filled = CHLA.fillna(-np.inf)\n",
    "    peak_idx = CHLA_filled.argmax(\"z\")       # (time, lat, lon) integer indices\n",
    "\n",
    "    CHLA_peak = CHLA.isel(z=peak_idx).where(valid)\n",
    "    CHLA_peak.name = \"CHLA_peak\"\n",
    "\n",
    "    CHLA_peak_depth = z_center.isel(z=peak_idx).where(valid)\n",
    "    CHLA_peak_depth.name = \"CHLA_peak_depth\"\n",
    "\n",
    "    # ---- Depth-weighted mean depth (center of mass) ----\n",
    "    num = (CHLA * z_center).sum(\"z\")\n",
    "    den = col_total\n",
    "    depth_cm = (num / den).where(valid)\n",
    "    depth_cm.name = \"CHLA_depth_center_of_mass\"\n",
    "\n",
    "    # ---- Attach derived fields to the dataset ----\n",
    "    ds[\"CHLA_int_0_200\"] = CHLA_int\n",
    "    ds[\"CHLA_peak\"] = CHLA_peak\n",
    "    ds[\"CHLA_peak_depth\"] = CHLA_peak_depth\n",
    "    ds[\"CHLA_depth_center_of_mass\"] = depth_cm\n",
    "    ds[\"z_thickness\"] = z_thick\n",
    "\n",
    "    # ---- Variable attributes ----\n",
    "    ds[\"CHLA\"].attrs.setdefault(\"units\", \"mg m-3\")\n",
    "    ds[\"CHLA\"].attrs.setdefault(\"long_name\", \"Chlorophyll-a concentration\")\n",
    "    ds[\"CHLA\"].attrs.setdefault(\n",
    "        \"description\",\n",
    "        \"BRT-derived chlorophyll-a profiles from PACE hyperspectral Rrs\",\n",
    "    )\n",
    "\n",
    "    ds[\"CHLA_int_0_200\"].attrs.update(\n",
    "        units=\"mg m-2\",\n",
    "        long_name=\"Depth-integrated chlorophyll-a\",\n",
    "        description=(\n",
    "            \"Vertical integral of CHLA over the available depth bins \"\n",
    "            \"(nominally 0–200 m; actual range defined by z_start/z_end).\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    ds[\"CHLA_peak\"].attrs.update(\n",
    "        units=\"mg m-3\",\n",
    "        long_name=\"Peak chlorophyll-a concentration in the water column\",\n",
    "        description=\"Maximum CHLA value over depth at each (time, lat, lon).\",\n",
    "    )\n",
    "\n",
    "    ds[\"CHLA_peak_depth\"].attrs.update(\n",
    "        units=\"m\",\n",
    "        long_name=\"Depth of peak chlorophyll-a\",\n",
    "        positive=\"down\",\n",
    "        description=(\n",
    "            \"Depth (bin center) where CHLA is maximal in the water column \"\n",
    "            \"at each (time, lat, lon).\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    ds[\"CHLA_depth_center_of_mass\"].attrs.update(\n",
    "        units=\"m\",\n",
    "        long_name=\"Chlorophyll-a depth center of mass\",\n",
    "        positive=\"down\",\n",
    "        description=(\n",
    "            \"Depth of the chlorophyll-a center of mass, computed as \"\n",
    "            \"sum_z(CHLA * z) / sum_z(CHLA).\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    ds[\"z_thickness\"].attrs.update(\n",
    "        units=\"m\",\n",
    "        long_name=\"Layer thickness\",\n",
    "        description=(\n",
    "            \"Thickness of each vertical bin used for depth integration. \"\n",
    "            \"Derived from z_end - z_start when available; otherwise set to a \"\n",
    "            \"uniform nominal thickness.\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return ds\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# Worker-side function: process ONE granule/day\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "def process_one_granule(\n",
    "    res,\n",
    "    lat_chunk=LAT_CHUNK,\n",
    "    lon_chunk=LON_CHUNK,\n",
    "    bucket_name=BUCKET_NAME,\n",
    "    destination_prefix=DESTINATION_PREFIX,\n",
    "    force_rerun=FORCE_RERUN,\n",
    "):\n",
    "    \"\"\"\n",
    "    Run the full pipeline for a single PACE L3M Rrs DAY granule:\n",
    "      - check if daily NetCDF already exists in GCS (skip if so and not force_rerun)\n",
    "      - download Rrs via earthaccess\n",
    "      - run BRT CHLA(z) prediction\n",
    "      - compute derived metrics\n",
    "      - write daily NetCDF to local temp\n",
    "      - upload NetCDF to GCS\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Message with status and GCS path (or SKIP info).\n",
    "    \"\"\"\n",
    "    import earthaccess\n",
    "    import xarray as xr\n",
    "    import pandas as pd\n",
    "    from google.cloud import storage\n",
    "    from pathlib import Path\n",
    "    import tempfile\n",
    "\n",
    "    # day as ISO string from UMM\n",
    "    day_iso = res[\"umm\"][\"TemporalExtent\"][\"RangeDateTime\"][\"BeginningDateTime\"]\n",
    "    day = pd.to_datetime(day_iso)\n",
    "    day_str = day.strftime(\"%Y%m%d\")\n",
    "\n",
    "    # Check if this day's file already exists in GCS\n",
    "    storage_client = storage.Client(project=\"noaa-gcs-public-data\")\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob_path = f\"{destination_prefix}/chla_z_{day_str}.nc\"\n",
    "    blob = bucket.blob(blob_path)\n",
    "\n",
    "    if blob.exists() and not force_rerun:\n",
    "        msg = f\"[{day_str}] SKIP (already exists at gs://{bucket_name}/{blob_path})\"\n",
    "        print(msg)\n",
    "        return msg\n",
    "\n",
    "    # Earthaccess auth on worker; assumes ~/.netrc is visible to worker\n",
    "    auth = earthaccess.login(persist=True)\n",
    "\n",
    "    # Open Rrs dataset for this granule\n",
    "    files = earthaccess.open([res], auth=auth, pqdm_kwargs={\"disable\": True})\n",
    "    rrs_ds = xr.open_dataset(files[0])\n",
    "\n",
    "    try:\n",
    "        # Rrs for that day\n",
    "        if \"time\" in rrs_ds.dims:\n",
    "            R = rrs_ds[\"Rrs\"].sel(time=day).squeeze(\"time\")\n",
    "        else:\n",
    "            R = rrs_ds[\"Rrs\"]\n",
    "        R = R.transpose(\"lat\", \"lon\", \"wavelength\")\n",
    "\n",
    "        # CHLA(z) prediction for this day (uses bundle.predict)\n",
    "        pred = bundle.predict(\n",
    "            R,\n",
    "            brt_models=bundle.model,\n",
    "            feature_cols=bundle.meta[\"feature_cols\"],\n",
    "            consts={\"solar_hour\": 0, \"type\": 1},\n",
    "            chunk_size_lat=100,\n",
    "            time=day.to_datetime64(),\n",
    "            z_name=\"z\",\n",
    "            silent=True,\n",
    "        )  # (time=1, z, lat, lon), float32\n",
    "\n",
    "        ds_day = build_chla_profile_dataset(pred)\n",
    "\n",
    "        # Add/override metadata\n",
    "        ds_day[\"CHLA\"].attrs.update(\n",
    "            units=\"mg m-3\",\n",
    "            long_name=\"Chlorophyll-a concentration\",\n",
    "            description=\"BRT-derived CHLA profiles from PACE hyperspectral Rrs\",\n",
    "        )\n",
    "        ds_day[\"z\"].attrs.update(units=\"m\", long_name=\"depth (bin center)\")\n",
    "        ds_day[\"lat\"].attrs.update(units=\"degrees_north\")\n",
    "        ds_day[\"lon\"].attrs.update(units=\"degrees_east\")\n",
    "        ds_day.attrs[\"source\"] = \"BRT model trained on BGC-Argo + OOI matchups\"\n",
    "        ds_day.attrs[\"model_bundle\"] = Path(BUNDLE_PATH).name\n",
    "\n",
    "        # Write to local temporary NetCDF\n",
    "        tmp_dir = Path(tempfile.gettempdir())\n",
    "        local_path = tmp_dir / f\"chla_z_{day_str}.nc\"\n",
    "\n",
    "        encoding = {\n",
    "            \"CHLA\": {\n",
    "                \"dtype\": \"float32\",\n",
    "                \"zlib\": True,\n",
    "                \"complevel\": 4,\n",
    "                \"chunksizes\": (1, ds_day.sizes[\"z\"], lat_chunk, lon_chunk),\n",
    "            }\n",
    "        }\n",
    "\n",
    "        ds_day.to_netcdf(\n",
    "            local_path,\n",
    "            engine=\"h5netcdf\",\n",
    "            encoding=encoding,\n",
    "        )\n",
    "\n",
    "        # Upload to GCS\n",
    "        blob.upload_from_filename(str(local_path))\n",
    "        local_path.unlink(missing_ok=True)\n",
    "\n",
    "        gcs_url = f\"gs://{bucket_name}/{blob_path}\"\n",
    "        msg = f\"[{day_str}] WROTE {gcs_url}\"\n",
    "        print(msg)\n",
    "        return msg\n",
    "\n",
    "    finally:\n",
    "        rrs_ds.close()\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# DRIVER: search granules, filter, and dispatch via Dask-Gateway\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "def main():\n",
    "    # 1. Earthaccess login on client\n",
    "    auth = earthaccess.login(persist=True)\n",
    "    if not auth.authenticated:\n",
    "        raise RuntimeError(\"earthaccess login failed\")\n",
    "\n",
    "    # 2. Search PACE L3M Rrs daily granules\n",
    "    rrs_results = earthaccess.search_data(\n",
    "        short_name=\"PACE_OCI_L3M_RRS\",\n",
    "        granule_name=\"*.DAY.*.4km.nc\",\n",
    "    )\n",
    "\n",
    "    # 3. Optional date filtering\n",
    "    def granule_day(res):\n",
    "        iso = res[\"umm\"][\"TemporalExtent\"][\"RangeDateTime\"][\"BeginningDateTime\"]\n",
    "        return pd.to_datetime(iso)\n",
    "\n",
    "    if START_DATE is not None:\n",
    "        start = pd.to_datetime(START_DATE)\n",
    "        rrs_results = [r for r in rrs_results if granule_day(r) >= start]\n",
    "\n",
    "    if END_DATE is not None:\n",
    "        end = pd.to_datetime(END_DATE)\n",
    "        rrs_results = [r for r in rrs_results if granule_day(r) <= end]\n",
    "\n",
    "    print(f\"Found {len(rrs_results)} DAY granules after date filter.\")\n",
    "\n",
    "    if not rrs_results:\n",
    "        print(\"Nothing to do.\")\n",
    "        return\n",
    "\n",
    "    # 4. Dask-Gateway cluster setup\n",
    "    gateway = Gateway()\n",
    "    options = gateway.cluster_options()\n",
    "\n",
    "    # These attributes may or may not exist depending on your deployment;\n",
    "    # if they don't, comment these two lines out and set resources via the UI.\n",
    "    if hasattr(options, \"worker_cores\"):\n",
    "        options.worker_cores = WORKER_CORES\n",
    "    if hasattr(options, \"worker_memory\"):\n",
    "        options.worker_memory = WORKER_MEMORY\n",
    "\n",
    "    cluster = gateway.new_cluster(options)\n",
    "    cluster.adapt(minimum=MIN_WORKERS, maximum=MAX_WORKERS)\n",
    "\n",
    "    client = cluster.get_client()\n",
    "    print(cluster)\n",
    "    print(client)\n",
    "\n",
    "    # 5. Dispatch one task per granule\n",
    "    futures = client.map(\n",
    "        process_one_granule,\n",
    "        rrs_results,\n",
    "    )\n",
    "\n",
    "    # 6. Wait for completion and collect messages\n",
    "    results = client.gather(futures)\n",
    "    print(\"Pipeline complete. Task summaries:\")\n",
    "    for r in results:\n",
    "        print(\"  \", r)\n",
    "\n",
    "    client.close()\n",
    "    cluster.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d136c4fb-1100-4c71-95d8-fc365ab7e680",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
