{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdbc84e2-b0db-4efe-8ae2-00d2555a2114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-storage in /srv/conda/envs/notebook/lib/python3.11/site-packages (3.7.0)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.26.1 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from google-cloud-storage) (2.43.0)\n",
      "Requirement already satisfied: google-api-core<3.0.0,>=2.27.0 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from google-cloud-storage) (2.28.1)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0,>=2.4.2 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from google-cloud-storage) (2.5.0)\n",
      "Requirement already satisfied: google-resumable-media<3.0.0,>=2.7.2 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from google-cloud-storage) (2.8.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.22.0 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from google-cloud-storage) (2.32.5)\n",
      "Requirement already satisfied: google-crc32c<2.0.0,>=1.1.3 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from google-cloud-storage) (1.7.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from google-api-core<3.0.0,>=2.27.0->google-cloud-storage) (1.72.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from google-api-core<3.0.0,>=2.27.0->google-cloud-storage) (6.33.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from google-api-core<3.0.0,>=2.27.0->google-cloud-storage) (1.26.1)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from google-auth<3.0.0,>=2.26.1->google-cloud-storage) (6.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from google-auth<3.0.0,>=2.26.1->google-cloud-storage) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from google-auth<3.0.0,>=2.26.1->google-cloud-storage) (4.9.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from requests<3.0.0,>=2.22.0->google-cloud-storage) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from requests<3.0.0,>=2.22.0->google-cloud-storage) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from requests<3.0.0,>=2.22.0->google-cloud-storage) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from requests<3.0.0,>=2.22.0->google-cloud-storage) (2025.8.3)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.26.1->google-cloud-storage) (0.6.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install google-cloud-storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f07560f-cf20-4355-bc97-93696390891d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d21f8ada-db1e-4335-9cd0-e08c826bad01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_google_cloud_storage():\n",
    "    \"\"\"Install google-cloud-storage on the worker if it's missing.\"\"\"\n",
    "    import importlib\n",
    "    import subprocess\n",
    "    import sys\n",
    "\n",
    "    try:\n",
    "        importlib.import_module(\"google.cloud.storage\")\n",
    "    except ImportError:\n",
    "        subprocess.check_call(\n",
    "            [sys.executable, \"-m\", \"pip\", \"install\", \"google-cloud-storage\"]\n",
    "        )\n",
    "ensure_google_cloud_storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcc9cb0-d63f-4d98-8e35-d19dba48467f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "585c525e-4fd4-4d4b-af9a-a263a7264298",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parallel daily CHLA(z) production using Dask-Gateway.\n",
    "\n",
    "- Searches PACE L3M Rrs DAY granules via earthaccess\n",
    "- For each granule/day:\n",
    "    * downloads Rrs\n",
    "    * runs BRT CHLA(z) prediction\n",
    "    * computes integrated/peak metrics\n",
    "    * writes a daily NetCDF locally\n",
    "    * uploads to GCS\n",
    "- Skips days that already exist in GCS unless FORCE_RERUN=True\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import earthaccess\n",
    "from google.cloud import storage\n",
    "from dask_gateway import Gateway\n",
    "from dask.distributed import Client\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# CONFIG\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "# Path to your saved ML bundle (zip) – adjust as needed\n",
    "BUNDLE_PATH = \"models/brt_chla_profiles_bundle.zip\"\n",
    "BUNDLE_FILENAME = Path(BUNDLE_PATH).name  # \"brt_chla_profiles_bundle.zip\"\n",
    "\n",
    "# GCS target\n",
    "BUCKET_NAME = \"nmfs_odp_nwfsc\"\n",
    "DESTINATION_PREFIX = \"CB/fish-pace-datasets/chla-z/netcdf\"\n",
    "\n",
    "# Dask-Gateway settings\n",
    "MIN_WORKERS = 4\n",
    "MAX_WORKERS = 12\n",
    "WORKER_CORES = 4\n",
    "WORKER_MEMORY = \"32GiB\"\n",
    "\n",
    "# Spatial chunking for NetCDF output\n",
    "LAT_CHUNK = 100\n",
    "LON_CHUNK = 100\n",
    "\n",
    "# Rerun control: if False, skip days that already exist in GCS\n",
    "FORCE_RERUN = False\n",
    "\n",
    "# Optional date filtering for rrs_results (None = no filter)\n",
    "START_DATE = None  # e.g. \"2024-03-01\"\n",
    "END_DATE   = None  # e.g. \"2024-04-30\"\n",
    "\n",
    "#START_DATE = \"2024-04-01\"\n",
    "#END_DATE   = \"2024-04-02\"\n",
    "\n",
    "import netrc\n",
    "import json\n",
    "\n",
    "netrc_path = os.path.expanduser(\"~/.netrc\")\n",
    "auth = netrc.netrc(netrc_path)\n",
    "login, account, password = auth.authenticators(\"urs.earthdata.nasa.gov\")\n",
    "ED_USER = login\n",
    "ED_PASS = password\n",
    "with open(\"/home/jovyan/.config/gcloud/application_default_credentials.json\") as f:\n",
    "    GCP_SA_JSON = f.read()\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# Helper: load ML bundle and build CHLA profile dataset\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "# Ensure ml_utils is available\n",
    "if not os.path.exists(\"ml_utils.py\"):\n",
    "    import subprocess\n",
    "    subprocess.run(\n",
    "        [\n",
    "            \"wget\",\n",
    "            \"-q\",\n",
    "            \"https://raw.githubusercontent.com/fish-pace/chla-z-modeling/main/ml_utils.py\",\n",
    "        ],\n",
    "        check=True,\n",
    "    )\n",
    "\n",
    "import ml_utils as mu  # noqa: E402\n",
    "\n",
    "# Load the bundle once on the client side; workers will receive it via pickling\n",
    "# DELETE\n",
    "# bundle = mu.load_ml_bundle(BUNDLE_PATH)\n",
    "\n",
    "#######################\n",
    "# - Helper\n",
    "#######################\n",
    "def ensure_google_cloud_storage():\n",
    "    \"\"\"Install google-cloud-storage on the worker if it's missing.\"\"\"\n",
    "    import importlib\n",
    "    import subprocess\n",
    "    import sys\n",
    "\n",
    "    try:\n",
    "        importlib.import_module(\"google.cloud.storage\")\n",
    "    except ImportError:\n",
    "        subprocess.check_call(\n",
    "            [sys.executable, \"-m\", \"pip\", \"install\", \"google-cloud-storage\"]\n",
    "        )\n",
    "\n",
    "\n",
    "def build_chla_profile_dataset(CHLA: xr.DataArray) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Given CHLA(time, z, lat, lon), compute derived metrics and\n",
    "    return an xr.Dataset suitable for writing to Zarr/NetCDF.\n",
    "    \"\"\"\n",
    "    # Start from CHLA's own dataset so its coords (including z_start/z_end) win\n",
    "    ds = CHLA.to_dataset(name=\"CHLA\")\n",
    "\n",
    "    # ---- Layer thickness (z dimension) ----\n",
    "    z_start = CHLA.coords.get(\"z_start\", None)\n",
    "    z_end   = CHLA.coords.get(\"z_end\", None)\n",
    "\n",
    "    if (z_start is not None) and (z_end is not None):\n",
    "        z_thick = (z_end - z_start).rename(\"z_thickness\")   # (z)\n",
    "    else:\n",
    "        # fallback: uniform layer thickness, e.g. 10 m\n",
    "        z_thick = xr.full_like(CHLA[\"z\"], 10.0).rename(\"z_thickness\")\n",
    "\n",
    "    z_center = CHLA[\"z\"]\n",
    "\n",
    "    # total CHLA in column (used for validity + center-of-mass)\n",
    "    col_total = CHLA.sum(\"z\")          # (time, lat, lon)\n",
    "    valid = col_total > 0              # True where there is some CHLA\n",
    "\n",
    "    # ---- Integrated CHLA (nominal 0–200 m; actual range = z extent) ----\n",
    "    CHLA_int = (CHLA * z_thick).sum(\"z\")\n",
    "    CHLA_int = CHLA_int.where(valid)\n",
    "    CHLA_int.name = \"CHLA_int_0_200\"\n",
    "\n",
    "    # ---- Peak value and depth (NaN-safe) ----\n",
    "    CHLA_filled = CHLA.fillna(-np.inf)\n",
    "    peak_idx = CHLA_filled.argmax(\"z\")       # (time, lat, lon) integer indices\n",
    "\n",
    "    CHLA_peak = CHLA.isel(z=peak_idx).where(valid)\n",
    "    CHLA_peak.name = \"CHLA_peak\"\n",
    "\n",
    "    CHLA_peak_depth = z_center.isel(z=peak_idx).where(valid)\n",
    "    CHLA_peak_depth.name = \"CHLA_peak_depth\"\n",
    "\n",
    "    # ---- Depth-weighted mean depth (center of mass) ----\n",
    "    num = (CHLA * z_center).sum(\"z\")\n",
    "    den = col_total\n",
    "    depth_cm = (num / den).where(valid)\n",
    "    depth_cm.name = \"CHLA_depth_center_of_mass\"\n",
    "\n",
    "    # ---- Attach derived fields to the dataset ----\n",
    "    ds[\"CHLA_int_0_200\"] = CHLA_int\n",
    "    ds[\"CHLA_peak\"] = CHLA_peak\n",
    "    ds[\"CHLA_peak_depth\"] = CHLA_peak_depth\n",
    "    ds[\"CHLA_depth_center_of_mass\"] = depth_cm\n",
    "    ds[\"z_thickness\"] = z_thick\n",
    "\n",
    "    # ---- Variable attributes ----\n",
    "    ds[\"CHLA\"].attrs.setdefault(\"units\", \"mg m-3\")\n",
    "    ds[\"CHLA\"].attrs.setdefault(\"long_name\", \"Chlorophyll-a concentration\")\n",
    "    ds[\"CHLA\"].attrs.setdefault(\n",
    "        \"description\",\n",
    "        \"BRT-derived chlorophyll-a profiles from PACE hyperspectral Rrs\",\n",
    "    )\n",
    "\n",
    "    ds[\"CHLA_int_0_200\"].attrs.update(\n",
    "        units=\"mg m-2\",\n",
    "        long_name=\"Depth-integrated chlorophyll-a\",\n",
    "        description=(\n",
    "            \"Vertical integral of CHLA over the available depth bins \"\n",
    "            \"(nominally 0–200 m; actual range defined by z_start/z_end).\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    ds[\"CHLA_peak\"].attrs.update(\n",
    "        units=\"mg m-3\",\n",
    "        long_name=\"Peak chlorophyll-a concentration in the water column\",\n",
    "        description=\"Maximum CHLA value over depth at each (time, lat, lon).\",\n",
    "    )\n",
    "\n",
    "    ds[\"CHLA_peak_depth\"].attrs.update(\n",
    "        units=\"m\",\n",
    "        long_name=\"Depth of peak chlorophyll-a\",\n",
    "        positive=\"down\",\n",
    "        description=(\n",
    "            \"Depth (bin center) where CHLA is maximal in the water column \"\n",
    "            \"at each (time, lat, lon).\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    ds[\"CHLA_depth_center_of_mass\"].attrs.update(\n",
    "        units=\"m\",\n",
    "        long_name=\"Chlorophyll-a depth center of mass\",\n",
    "        positive=\"down\",\n",
    "        description=(\n",
    "            \"Depth of the chlorophyll-a center of mass, computed as \"\n",
    "            \"sum_z(CHLA * z) / sum_z(CHLA).\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    ds[\"z_thickness\"].attrs.update(\n",
    "        units=\"m\",\n",
    "        long_name=\"Layer thickness\",\n",
    "        description=(\n",
    "            \"Thickness of each vertical bin used for depth integration. \"\n",
    "            \"Derived from z_end - z_start when available; otherwise set to a \"\n",
    "            \"uniform nominal thickness.\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return ds\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# Worker-side function: process ONE granule/day\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "def process_one_granule(\n",
    "    res,\n",
    "    lat_chunk=LAT_CHUNK,\n",
    "    lon_chunk=LON_CHUNK,\n",
    "    bucket_name=BUCKET_NAME,\n",
    "    destination_prefix=DESTINATION_PREFIX,\n",
    "    force_rerun=FORCE_RERUN,\n",
    "    ed_username=ED_USER,\n",
    "    ed_password=ED_PASS,\n",
    "    gcp_sa_json=GCP_SA_JSON,\n",
    "    bundle_filename=BUNDLE_FILENAME,\n",
    "):\n",
    "    import os\n",
    "    import tempfile\n",
    "    import earthaccess\n",
    "    import xarray as xr\n",
    "    import pandas as pd\n",
    "    from pathlib import Path\n",
    "    import ml_utils as mu  # <- now workers can import this\n",
    "\n",
    "    # --- ensure google-cloud-storage is available on THIS worker ---\n",
    "    import importlib\n",
    "    import subprocess\n",
    "    import sys\n",
    "    try:\n",
    "        importlib.import_module(\"google.cloud.storage\")\n",
    "    except ImportError:\n",
    "        subprocess.check_call(\n",
    "            [sys.executable, \"-m\", \"pip\", \"install\", \"google-cloud-storage\"]\n",
    "        )\n",
    "\n",
    "    from google.cloud import storage  # now this should succeed\n",
    "\n",
    "    # --- locate the bundle file next to ml_utils.py ---\n",
    "    bundle_path = Path(mu.__file__).with_name(bundle_filename)\n",
    "    # just to be extra defensive:\n",
    "    if not bundle_path.exists():\n",
    "        raise FileNotFoundError(f\"Bundle not found at {bundle_path}\")\n",
    "\n",
    "    bundle = mu.load_ml_bundle(str(bundle_path))\n",
    "    \n",
    "    # Load bundle on the worker from the uploaded zip file\n",
    "    #bundle = mu.load_ml_bundle(bundle_filename)\n",
    "\n",
    "    # --- EARTHACCESS AUTH VIA ENV VARS (inside worker) ---\n",
    "    if ed_username is not None and ed_password is not None:\n",
    "        os.environ[\"EARTHDATA_USERNAME\"] = ed_username\n",
    "        os.environ[\"EARTHDATA_PASSWORD\"] = ed_password\n",
    "\n",
    "    auth = earthaccess.login(strategy=\"environment\", persist=False)\n",
    "\n",
    "    # --- GCP AUTH VIA JSON TEXT (inside worker) ---\n",
    "    import uuid\n",
    "\n",
    "    cred_path = None\n",
    "    if gcp_sa_json:\n",
    "        cred_path = os.path.join(tempfile.gettempdir(), f\"gcp_sa_worker_{uuid.uuid4().hex}.json\")\n",
    "        with open(cred_path, \"w\") as f:\n",
    "            f.write(gcp_sa_json)\n",
    "        os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = cred_path\n",
    "\n",
    "    # -------------------------------\n",
    "    #  Normal per-day pipeline below\n",
    "    # -------------------------------\n",
    "    day_iso = res[\"umm\"][\"TemporalExtent\"][\"RangeDateTime\"][\"BeginningDateTime\"]\n",
    "    day = pd.to_datetime(day_iso)\n",
    "    day_str = day.strftime(\"%Y%m%d\")\n",
    "\n",
    "    storage_client = storage.Client(project=\"noaa-gcs-public-data\")\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob_path = f\"{destination_prefix}/chla_z_{day_str}.nc\"\n",
    "    blob = bucket.blob(blob_path)\n",
    "\n",
    "    if blob.exists() and not force_rerun:\n",
    "        msg = f\"[{day_str}] SKIP (exists at gs://{bucket_name}/{blob_path})\"\n",
    "        print(msg)\n",
    "        return msg\n",
    "\n",
    "    files = earthaccess.open([res], pqdm_kwargs={\"disable\": True})\n",
    "    rrs_ds = xr.open_dataset(files[0])\n",
    "    # debug\n",
    "    # rrs_ds = rrs_ds.sel(lat=slice(40, 20), lon=slice(-70, -60) )\n",
    "\n",
    "    try:\n",
    "        if \"time\" in rrs_ds.dims:\n",
    "            R = rrs_ds[\"Rrs\"].sel(time=day).squeeze(\"time\")\n",
    "        else:\n",
    "            R = rrs_ds[\"Rrs\"]\n",
    "        R = R.transpose(\"lat\", \"lon\", \"wavelength\")\n",
    "\n",
    "        pred = bundle.predict(\n",
    "            R,\n",
    "            brt_models=bundle.model,\n",
    "            feature_cols=bundle.meta[\"feature_cols\"],\n",
    "            consts={\"solar_hour\": 0, \"type\": 1},\n",
    "            chunk_size_lat=100,\n",
    "            time=day.to_datetime64(),\n",
    "            z_name=\"z\",\n",
    "            silent=True,\n",
    "        )\n",
    "\n",
    "        ds_day = build_chla_profile_dataset(pred)\n",
    "\n",
    "        tmp_dir = Path(tempfile.gettempdir())\n",
    "        local_path = tmp_dir / f\"chla_z_{day_str}.nc\"\n",
    "\n",
    "        encoding = {\n",
    "            \"CHLA\": {\n",
    "                \"dtype\": \"float32\",\n",
    "                \"zlib\": True,\n",
    "                \"complevel\": 4,\n",
    "                \"chunksizes\": (1, ds_day.sizes[\"z\"], lat_chunk, lon_chunk),\n",
    "            }\n",
    "        }\n",
    "\n",
    "        ds_day.to_netcdf(local_path, engine=\"h5netcdf\", encoding=encoding)\n",
    "        blob.upload_from_filename(str(local_path))\n",
    "        local_path.unlink(missing_ok=True)\n",
    "\n",
    "        gcs_url = f\"gs://{bucket_name}/{blob_path}\"\n",
    "        msg = f\"[{day_str}] WROTE {gcs_url}\"\n",
    "        print(msg)\n",
    "        return msg\n",
    "\n",
    "    finally:\n",
    "        rrs_ds.close()\n",
    "        # optional: clean up the creds file\n",
    "        if cred_path is not None:\n",
    "            try:\n",
    "                os.remove(cred_path)\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# DRIVER: search granules, filter, and dispatch via Dask-Gateway\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "def main():\n",
    "    # 1. Earthaccess login on client\n",
    "    auth = earthaccess.login(strategy=\"netrc\", persist=True)\n",
    "    if not auth.authenticated:\n",
    "        raise RuntimeError(\"earthaccess login failed\")\n",
    "\n",
    "    # 2. Search PACE L3M Rrs daily granules\n",
    "    rrs_results = earthaccess.search_data(\n",
    "        short_name=\"PACE_OCI_L3M_RRS\",\n",
    "        granule_name=\"*.DAY.*.4km.nc\",\n",
    "        temporal=(START_DATE, END_DATE),\n",
    "    )\n",
    "\n",
    "    print(f\"Found {len(rrs_results)} DAY granules after date filter.\")\n",
    "    if not rrs_results:\n",
    "        print(\"Nothing to do.\")\n",
    "        return\n",
    "\n",
    "    # 4. Dask-Gateway cluster setup\n",
    "    gateway = Gateway()\n",
    "    options = gateway.cluster_options()\n",
    "    setattr(options, \"worker_resource_allocation\", '4CPU, 30.2Gi')\n",
    "    \n",
    "    cluster = gateway.new_cluster(options)\n",
    "    cluster.adapt(minimum=MIN_WORKERS, maximum=MAX_WORKERS)\n",
    "\n",
    "    client = cluster.get_client()\n",
    "    print(cluster)\n",
    "    print(client)\n",
    "\n",
    "    # Dashboard link (copy/paste into a browser tab)\n",
    "    print(\"Dask dashboard:\", client.dashboard_link)\n",
    "\n",
    "    # Make sure workers have needed files\n",
    "    client.upload_file(\"ml_utils.py\")\n",
    "    client.upload_file(BUNDLE_PATH)\n",
    "\n",
    "    # ensure google-cloud-storage is installed on every worker\n",
    "    client.run(ensure_google_cloud_storage)\n",
    "\n",
    "    # 5. Dispatch one task per granule\n",
    "    futures = client.map(process_one_granule, rrs_results)\n",
    "\n",
    "    # 6. Stream results as they complete (instead of blocking on gather)\n",
    "    from dask.distributed import as_completed\n",
    "\n",
    "    n = len(futures)\n",
    "    done = 0\n",
    "    errors = 0\n",
    "\n",
    "    try:\n",
    "        for fut in as_completed(futures):\n",
    "            try:\n",
    "                msg = fut.result()\n",
    "                done += 1\n",
    "                print(f\"[{done}/{n}] {msg}\")\n",
    "            except Exception as e:\n",
    "                errors += 1\n",
    "                done += 1\n",
    "                print(f\"[{done}/{n}] ERROR: {repr(e)}\")\n",
    "                # If you want to stop on first error, uncomment:\n",
    "                # raise\n",
    "    finally:\n",
    "        print(f\"Finished. Success={done - errors}, Errors={errors}\")\n",
    "        client.close()\n",
    "        cluster.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7b900c-9fe3-4cff-aef4-6aec8a7fa52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# took 10 hours for 560 files\n",
    "if __name__ == \"__main__\": main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50601fdb-1f7c-475b-908a-58ad6f40f757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 DAY granules after date filter.\n",
      "GatewayCluster<prod.0d2bdc79e25a43f3b9a184b8bb2c7020, status=running>\n",
      "<Client: 'tls://192.168.35.98:8786' processes=0 threads=0, memory=0 B>\n",
      "Dask dashboard: /services/dask-gateway/clusters/prod.0d2bdc79e25a43f3b9a184b8bb2c7020/status\n",
      "[1/3] [20250211] WROTE gs://nmfs_odp_nwfsc/CB/fish-pace-datasets/chla-z/netcdf/chla_z_20250211.nc\n",
      "[2/3] [20250210] WROTE gs://nmfs_odp_nwfsc/CB/fish-pace-datasets/chla-z/netcdf/chla_z_20250210.nc\n",
      "[3/3] [20250209] WROTE gs://nmfs_odp_nwfsc/CB/fish-pace-datasets/chla-z/netcdf/chla_z_20250209.nc\n",
      "Finished. Success=3, Errors=0\n"
     ]
    }
   ],
   "source": [
    "# need to rerun a few days\n",
    "START_DATE = '20250209'\n",
    "END_DATE   = '20250211'\n",
    "if __name__ == \"__main__\": main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89769d4-9f8d-439d-a575-87d930f2d0d6",
   "metadata": {},
   "source": [
    "# Process Zarr\n",
    "\n",
    "Should have baked this into the first pipeline. Alas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01dd0d59-2f8b-49de-8731-6baa0117a3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import re\n",
    "import uuid\n",
    "import tempfile\n",
    "\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import gcsfs\n",
    "import zarr\n",
    "\n",
    "from dask_gateway import Gateway\n",
    "from dask.distributed import as_completed\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# CONFIG\n",
    "# -----------------------\n",
    "TOKEN_PATH = \"/home/jovyan/.config/gcloud/application_default_credentials.json\"\n",
    "\n",
    "BUCKET = \"nmfs_odp_nwfsc\"\n",
    "NETCDF_PREFIX = \"CB/fish-pace-datasets/chla-z/netcdf\"\n",
    "NETCDF_PATTERN = f\"{BUCKET}/{NETCDF_PREFIX}/chla_z_*.nc\"\n",
    "\n",
    "# You said you deleted this prefix already. Keep it Zarr v2 only.\n",
    "ZARR_PATH = f\"gcs://{BUCKET}/CB/fish-pace-datasets/chla-z/zarr_v2\"\n",
    "\n",
    "LAT_CHUNK = 128\n",
    "LON_CHUNK = 128\n",
    "\n",
    "MIN_WORKERS = 4\n",
    "MAX_WORKERS = 12\n",
    "WORKER_RESOURCE = \"4CPU, 30.2Gi\"\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# HELPERS\n",
    "# -----------------------\n",
    "_date_re = re.compile(r\"chla_z_(\\d{8})\\.nc$\")\n",
    "\n",
    "\n",
    "def date_from_url(gcs_url: str) -> pd.Timestamp:\n",
    "    m = _date_re.search(gcs_url)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Could not parse date from: {gcs_url}\")\n",
    "    return pd.to_datetime(m.group(1), format=\"%Y%m%d\")\n",
    "\n",
    "\n",
    "def list_netcdf_urls(fs: gcsfs.GCSFileSystem) -> list[str]:\n",
    "    paths = sorted(fs.glob(NETCDF_PATTERN))\n",
    "    urls = [\"gcs://\" + p for p in paths]\n",
    "    urls = sorted(urls, key=date_from_url)\n",
    "    return urls\n",
    "\n",
    "\n",
    "def build_time_index(urls: list[str]) -> pd.DatetimeIndex:\n",
    "    times = pd.to_datetime([date_from_url(u) for u in urls]).astype(\"datetime64[ns]\")\n",
    "    return pd.DatetimeIndex(times, name=\"time\")\n",
    "\n",
    "\n",
    "def _chunk_spec(ds: xr.Dataset, lat_chunk: int, lon_chunk: int) -> dict:\n",
    "    spec: dict[str, int] = {}\n",
    "    if \"time\" in ds.dims:\n",
    "        spec[\"time\"] = 1\n",
    "    if \"z\" in ds.dims:\n",
    "        spec[\"z\"] = ds.sizes[\"z\"]  # keep full z together\n",
    "    if \"lat\" in ds.dims:\n",
    "        spec[\"lat\"] = lat_chunk\n",
    "    if \"lon\" in ds.dims:\n",
    "        spec[\"lon\"] = lon_chunk\n",
    "    return spec\n",
    "\n",
    "\n",
    "def read_sa_json(token_path: str = TOKEN_PATH) -> str:\n",
    "    with open(token_path, \"r\") as f:\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "def setup_gcp_on_worker(gcp_sa_json: str) -> str:\n",
    "    \"\"\"\n",
    "    Write SA json to a unique temp file and point GOOGLE_APPLICATION_CREDENTIALS at it.\n",
    "    Returns the path so caller can optionally delete it.\n",
    "    \"\"\"\n",
    "    cred_path = os.path.join(\n",
    "        tempfile.gettempdir(), f\"gcp_sa_worker_{uuid.uuid4().hex}.json\"\n",
    "    )\n",
    "    with open(cred_path, \"w\") as f:\n",
    "        f.write(gcp_sa_json)\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = cred_path\n",
    "    return cred_path\n",
    "\n",
    "\n",
    "def make_gcsfs_with_sa_json(gcp_sa_json: str) -> tuple[gcsfs.GCSFileSystem, str]:\n",
    "    \"\"\"\n",
    "    Create a GCSFileSystem using a temp SA json file on this machine (client or worker).\n",
    "    Uses token='google_default' so gcsfs reads GOOGLE_APPLICATION_CREDENTIALS.\n",
    "    \"\"\"\n",
    "    cred_path = setup_gcp_on_worker(gcp_sa_json)\n",
    "    fs = gcsfs.GCSFileSystem(token=\"google_default\")\n",
    "    return fs, cred_path\n",
    "\n",
    "\n",
    "def create_zarr_template_by_write_then_resize(\n",
    "    gcp_sa_json: str,\n",
    "    first_url: str,\n",
    "    times: pd.DatetimeIndex,\n",
    "    zarr_path: str,\n",
    "    lat_chunk: int,\n",
    "    lon_chunk: int,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Create Zarr v2 store safely by:\n",
    "      1) downloading/opening first NetCDF locally\n",
    "      2) writing it to Zarr (mode=\"w\") -> creates arrays/metadata (v2)\n",
    "      3) resizing arrays along 'time' to full length (metadata-only)\n",
    "      4) writing full time coord values\n",
    "    \"\"\"\n",
    "    fs, cred_path = make_gcsfs_with_sa_json(gcp_sa_json)\n",
    "\n",
    "    tmp_dir = Path(\"/tmp/chla_zarr_template\")\n",
    "    tmp_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    local_nc = tmp_dir / Path(first_url).name\n",
    "    fs.get(first_url, str(local_nc))\n",
    "\n",
    "    ds0 = xr.open_dataset(local_nc, engine=\"h5netcdf\")\n",
    "    try:\n",
    "        if \"time\" in ds0.coords:\n",
    "            ds0 = ds0.assign_coords(time=ds0[\"time\"].astype(\"datetime64[ns]\"))\n",
    "\n",
    "        ds0 = ds0.chunk(_chunk_spec(ds0, lat_chunk, lon_chunk))\n",
    "\n",
    "        mapper = fs.get_mapper(zarr_path)\n",
    "\n",
    "        # 1) Write first day (creates the store cleanly)\n",
    "        ds0.to_zarr(\n",
    "            mapper,\n",
    "            mode=\"w\",\n",
    "            consolidated=False,\n",
    "            zarr_format=2,\n",
    "        )\n",
    "\n",
    "        # 2) Resize time on all arrays that have a 'time' dimension\n",
    "        full_len = len(times)\n",
    "        root = zarr.open_group(mapper, mode=\"r+\")\n",
    "\n",
    "        for name, arr in root.arrays():\n",
    "            dims = arr.attrs.get(\"_ARRAY_DIMENSIONS\", None)\n",
    "            if not dims or \"time\" not in dims:\n",
    "                continue\n",
    "            t_axis = list(dims).index(\"time\")\n",
    "            newshape = list(arr.shape)\n",
    "            newshape[t_axis] = full_len\n",
    "            if tuple(newshape) != arr.shape:\n",
    "                arr.resize(tuple(newshape))\n",
    "\n",
    "        # 3) Fill the time coordinate values (store as int64 ns)\n",
    "        time_int = times.values.astype(\"datetime64[ns]\").astype(\"int64\")\n",
    "        if \"time\" in root:\n",
    "            root[\"time\"][:] = time_int\n",
    "            root[\"time\"].attrs[\"_ARRAY_DIMENSIONS\"] = [\"time\"]\n",
    "\n",
    "    finally:\n",
    "        ds0.close()\n",
    "        local_nc.unlink(missing_ok=True)\n",
    "        try:\n",
    "            os.remove(cred_path)\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "\n",
    "def write_one_day_region(\n",
    "    gcs_url: str,\n",
    "    time_index: int,\n",
    "    zarr_path: str,\n",
    "    gcp_sa_json: str,\n",
    "    lat_chunk: int,\n",
    "    lon_chunk: int,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Worker task:\n",
    "    - write SA json to /tmp on worker and set GOOGLE_APPLICATION_CREDENTIALS\n",
    "    - download NC to local /tmp (no streaming)\n",
    "    - open local\n",
    "    - rechunk\n",
    "    - write into correct time slice via region writes\n",
    "    \"\"\"\n",
    "    from pathlib import Path\n",
    "    import os\n",
    "    import xarray as xr\n",
    "    import gcsfs\n",
    "\n",
    "    cred_path = setup_gcp_on_worker(gcp_sa_json)\n",
    "    fs = gcsfs.GCSFileSystem(token=\"google_default\")\n",
    "\n",
    "    tmp_dir = Path(\"/tmp/chla_nc_to_zarr_workers\")\n",
    "    tmp_dir.mkdir(parents=True, exist_ok=True)\n",
    "    local_nc = tmp_dir / Path(gcs_url).name\n",
    "\n",
    "    fs.get(gcs_url, str(local_nc))\n",
    "\n",
    "    ds = xr.open_dataset(local_nc, engine=\"h5netcdf\")\n",
    "    try:\n",
    "        if \"time\" in ds.coords:\n",
    "            ds = ds.assign_coords(time=ds[\"time\"].astype(\"datetime64[ns]\"))\n",
    "\n",
    "        ds = ds.chunk(_chunk_spec(ds, lat_chunk, lon_chunk))\n",
    "\n",
    "        region = {\"time\": slice(time_index, time_index + 1)}\n",
    "\n",
    "        ds.to_zarr(\n",
    "            fs.get_mapper(zarr_path),\n",
    "            mode=\"r+\",\n",
    "            region=region,\n",
    "            consolidated=False,\n",
    "            zarr_format=2,\n",
    "        )\n",
    "\n",
    "        return f\"OK {Path(gcs_url).name} -> time_index={time_index}\"\n",
    "\n",
    "    finally:\n",
    "        ds.close()\n",
    "        local_nc.unlink(missing_ok=True)\n",
    "        try:\n",
    "            os.remove(cred_path)\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "\n",
    "def consolidate_zarr(zarr_path: str, gcp_sa_json: str) -> None:\n",
    "    fs, cred_path = make_gcsfs_with_sa_json(gcp_sa_json)\n",
    "    try:\n",
    "        mapper = fs.get_mapper(zarr_path)\n",
    "        zarr.consolidate_metadata(mapper)\n",
    "    finally:\n",
    "        try:\n",
    "            os.remove(cred_path)\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# MAIN\n",
    "# -----------------------\n",
    "def zarr_main():\n",
    "    # Read SA JSON once on the client and pass the TEXT to workers\n",
    "    gcp_sa_json = read_sa_json(TOKEN_PATH)\n",
    "\n",
    "    # Use SA JSON on client too (don’t rely on file path existing in workers)\n",
    "    fs, cred_path = make_gcsfs_with_sa_json(gcp_sa_json)\n",
    "    try:\n",
    "        urls = list_netcdf_urls(fs)\n",
    "        if not urls:\n",
    "            raise RuntimeError(\"No NetCDF files found with pattern: \" + NETCDF_PATTERN)\n",
    "\n",
    "        print(\"nfiles:\", len(urls), \"first:\", urls[0])\n",
    "\n",
    "        times = build_time_index(urls)\n",
    "\n",
    "        print(\"Creating Zarr template (write first day, then resize time)…\")\n",
    "        create_zarr_template_by_write_then_resize(\n",
    "            gcp_sa_json=gcp_sa_json,\n",
    "            first_url=urls[0],\n",
    "            times=times,\n",
    "            zarr_path=ZARR_PATH,\n",
    "            lat_chunk=LAT_CHUNK,\n",
    "            lon_chunk=LON_CHUNK,\n",
    "        )\n",
    "        print(\"Template created:\", ZARR_PATH)\n",
    "\n",
    "        gateway = Gateway()\n",
    "        options = gateway.cluster_options()\n",
    "        setattr(options, \"worker_resource_allocation\", WORKER_RESOURCE)\n",
    "\n",
    "        cluster = gateway.new_cluster(options)\n",
    "        cluster.adapt(minimum=MIN_WORKERS, maximum=MAX_WORKERS)\n",
    "        client = cluster.get_client()\n",
    "\n",
    "        print(cluster)\n",
    "        print(client)\n",
    "        print(\"Dask dashboard:\", client.dashboard_link)\n",
    "\n",
    "        # Optional quick auth sanity check on workers (fails fast if creds not working)\n",
    "        def _worker_ls(smoke_prefix: str, gcp_sa_json: str) -> int:\n",
    "            import gcsfs, os, tempfile, uuid\n",
    "            cred = os.path.join(tempfile.gettempdir(), f\"gcp_sa_worker_{uuid.uuid4().hex}.json\")\n",
    "            with open(cred, \"w\") as f:\n",
    "                f.write(gcp_sa_json)\n",
    "            os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = cred\n",
    "            fsw = gcsfs.GCSFileSystem(token=\"google_default\")\n",
    "            out = fsw.ls(smoke_prefix)\n",
    "            try:\n",
    "                os.remove(cred)\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "            return len(out)\n",
    "\n",
    "        try:\n",
    "            nls = client.run(_worker_ls, f\"{BUCKET}/CB/fish-pace-datasets/chla-z\", gcp_sa_json)\n",
    "            print(\"Worker auth smoke test (ls counts):\", nls)\n",
    "        except Exception as e:\n",
    "            print(\"Worker auth smoke test FAILED:\", repr(e))\n",
    "            raise\n",
    "\n",
    "        futures = []\n",
    "        for idx, url in enumerate(urls):\n",
    "            fut = client.submit(\n",
    "                write_one_day_region,\n",
    "                url,\n",
    "                idx,\n",
    "                ZARR_PATH,\n",
    "                gcp_sa_json,   # <-- JSON TEXT to worker\n",
    "                LAT_CHUNK,\n",
    "                LON_CHUNK,\n",
    "                pure=False,\n",
    "            )\n",
    "            futures.append(fut)\n",
    "\n",
    "        n = len(futures)\n",
    "        done = 0\n",
    "        errors = 0\n",
    "\n",
    "        try:\n",
    "            for fut in as_completed(futures):\n",
    "                done += 1\n",
    "                try:\n",
    "                    msg = fut.result()\n",
    "                    print(f\"[{done}/{n}] {msg}\")\n",
    "                except Exception as e:\n",
    "                    errors += 1\n",
    "                    print(f\"[{done}/{n}] ERROR: {repr(e)}\")\n",
    "\n",
    "            print(f\"Finished writing. Success={n - errors}, Errors={errors}\")\n",
    "\n",
    "            print(\"Consolidating Zarr metadata…\")\n",
    "            consolidate_zarr(ZARR_PATH, gcp_sa_json)\n",
    "            print(\"Done + consolidated.\")\n",
    "\n",
    "        finally:\n",
    "            client.close()\n",
    "            cluster.close()\n",
    "\n",
    "    finally:\n",
    "        try:\n",
    "            os.remove(cred_path)\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bc9f4b-d4f1-4268-9e9f-2ed367c45ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/google/auth/_default.py:108: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nfiles: 560 first: gcs://nmfs_odp_nwfsc/CB/fish-pace-datasets/chla-z/netcdf/chla_z_20240305.nc\n",
      "Creating Zarr template (write first day, then resize time)…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/google/auth/_default.py:108: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n",
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/google/auth/_default.py:108: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Template created: gcs://nmfs_odp_nwfsc/CB/fish-pace-datasets/chla-z/zarr_v2\n",
      "GatewayCluster<prod.e6cdcf3cb77d41a49e9f7486bbcbe91a, status=running>\n",
      "<Client: 'tls://192.168.63.220:8786' processes=0 threads=0, memory=0 B>\n",
      "Dask dashboard: /services/dask-gateway/clusters/prod.e6cdcf3cb77d41a49e9f7486bbcbe91a/status\n",
      "Worker auth smoke test (ls counts): {}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    zarr_main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8972d751-6dcb-4086-a1e0-97aa88d66eed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
