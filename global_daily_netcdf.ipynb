{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fe6c8b9-2581-4dde-9447-072103be47bd",
   "metadata": {},
   "source": [
    "# Create daily global netcdfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c0e36d3-cba1-439f-a35f-d5c9785385b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Core data handling and plotting libraries ---\n",
    "import earthaccess\n",
    "import xarray as xr       # for working with labeled multi-dimensional arrays\n",
    "import numpy as np        # for numerical operations on arrays\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt  # for creating plots\n",
    "import cartopy.crs as ccrs\n",
    "import sklearn\n",
    "# --- Custom python functions ---\n",
    "import os, importlib\n",
    "# Looks to see if you have the file already and if not, downloads from GitHub\n",
    "if not os.path.exists(\"ml_utils.py\"):\n",
    "    !wget -q https://raw.githubusercontent.com/fish-pace/2025-tutorials/main/ml_utils.py\n",
    "\n",
    "import ml_utils as mu\n",
    "importlib.reload(mu)\n",
    "# core stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from scipy.ndimage import uniform_filter1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c64a0d52-a78d-4efd-9241-46ba53689480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-cloud-storage\n",
      "  Using cached google_cloud_storage-3.7.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting google-auth<3.0.0,>=2.26.1 (from google-cloud-storage)\n",
      "  Using cached google_auth-2.43.0-py2.py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting google-api-core<3.0.0,>=2.27.0 (from google-cloud-storage)\n",
      "  Using cached google_api_core-2.28.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting google-cloud-core<3.0.0,>=2.4.2 (from google-cloud-storage)\n",
      "  Using cached google_cloud_core-2.5.0-py3-none-any.whl.metadata (3.1 kB)\n",
      "Collecting google-resumable-media<3.0.0,>=2.7.2 (from google-cloud-storage)\n",
      "  Using cached google_resumable_media-2.8.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.22.0 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from google-cloud-storage) (2.32.5)\n",
      "Collecting google-crc32c<2.0.0,>=1.1.3 (from google-cloud-storage)\n",
      "  Using cached google_crc32c-1.7.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core<3.0.0,>=2.27.0->google-cloud-storage)\n",
      "  Using cached googleapis_common_protos-1.72.0-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 (from google-api-core<3.0.0,>=2.27.0->google-cloud-storage)\n",
      "  Using cached protobuf-6.33.2-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Collecting proto-plus<2.0.0,>=1.22.3 (from google-api-core<3.0.0,>=2.27.0->google-cloud-storage)\n",
      "  Using cached proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from google-auth<3.0.0,>=2.26.1->google-cloud-storage) (6.2.0)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3.0.0,>=2.26.1->google-cloud-storage)\n",
      "  Using cached pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3.0.0,>=2.26.1->google-cloud-storage)\n",
      "  Using cached rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from requests<3.0.0,>=2.22.0->google-cloud-storage) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from requests<3.0.0,>=2.22.0->google-cloud-storage) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from requests<3.0.0,>=2.22.0->google-cloud-storage) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from requests<3.0.0,>=2.22.0->google-cloud-storage) (2025.8.3)\n",
      "Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.26.1->google-cloud-storage)\n",
      "  Using cached pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Using cached google_cloud_storage-3.7.0-py3-none-any.whl (303 kB)\n",
      "Using cached google_api_core-2.28.1-py3-none-any.whl (173 kB)\n",
      "Using cached google_auth-2.43.0-py2.py3-none-any.whl (223 kB)\n",
      "Using cached google_cloud_core-2.5.0-py3-none-any.whl (29 kB)\n",
      "Using cached google_crc32c-1.7.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
      "Using cached google_resumable_media-2.8.0-py3-none-any.whl (81 kB)\n",
      "Using cached googleapis_common_protos-1.72.0-py3-none-any.whl (297 kB)\n",
      "Using cached proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "Using cached protobuf-6.33.2-cp39-abi3-manylinux2014_x86_64.whl (323 kB)\n",
      "Using cached rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Using cached pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Installing collected packages: pyasn1, protobuf, google-crc32c, rsa, pyasn1-modules, proto-plus, googleapis-common-protos, google-resumable-media, google-auth, google-api-core, google-cloud-core, google-cloud-storage\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/12\u001b[0m [google-cloud-storage]gle-cloud-storage]tos]\n",
      "\u001b[1A\u001b[2KSuccessfully installed google-api-core-2.28.1 google-auth-2.43.0 google-cloud-core-2.5.0 google-cloud-storage-3.7.0 google-crc32c-1.7.1 google-resumable-media-2.8.0 googleapis-common-protos-1.72.0 proto-plus-1.26.1 protobuf-6.33.2 pyasn1-0.6.1 pyasn1-modules-0.4.2 rsa-4.9.1\n"
     ]
    }
   ],
   "source": [
    "!pip install google-cloud-storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c288a3-9842-4c27-9030-55adb0e18238",
   "metadata": {},
   "source": [
    "## Read in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17bd33ba-7d82-472f-8b07-5ac9ee33e8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded ML bundle from: models/brt_chla_profiles_bundle.zip\n",
      "  model_kind : pickle\n",
      "  model_type : collection (dict), n_submodels=20\n",
      "  example key: CHLA_0_10\n",
      "  target     : log10_CHLA_A_B depth bins\n",
      "  features   : 174 columns\n",
      "  train/test : 4408 / 1102 rows\n",
      "  dataset    : 5510 rows stored in bundle\n",
      "\n",
      "Usage example (Python):\n",
      "  bundle = load_ml_bundle('path/to/bundle.zip')\n",
      "  # Predict using helper 'predict_all_depths_for_day'\n",
      "  # Example: predict all depths for one day from a BRF dataset R\n",
      "  pred = bundle.predict(\n",
      "      R_dataset,                  # xr.DataArray/xr.Dataset with lat/lon + predictors\n",
      "      brt_models=bundle.model,    # dict of models by depth bin\n",
      "      feature_cols=bundle.meta['feature_cols'],\n",
      "      consts={'solar_hour': 12.0, 'type': 1},\n",
      "  )  # -> e.g. CHLA(time?, z, lat, lon)\n",
      "\n",
      "  # Plot using helper 'make_plot_pred_map'\n",
      "  fig, ax = bundle.plot(pred_da, pred_label='Prediction')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Load model\n",
    "bundle = mu.load_ml_bundle(\"models/brt_chla_profiles_bundle.zip\")\n",
    "brt_models = bundle.model\n",
    "meta = bundle.meta\n",
    "rrs_cols = meta[\"rrs_cols\"]\n",
    "chl_cols = meta[\"y_col\"]\n",
    "extra = meta[\"extra_cols\"]\n",
    "dataset = bundle.data[\"dataset\"]\n",
    "train_idx = bundle.data[\"train_idx\"]\n",
    "test_idx = bundle.data[\"test_idx\"]\n",
    "X_train = bundle.data[\"X_train\"]\n",
    "X_test = bundle.data[\"X_test\"]\n",
    "y_train_all = bundle.data[\"y_train\"]\n",
    "y_test_all = bundle.data[\"y_test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b51f99-9948-4bb2-9f86-7635f0a8d6bb",
   "metadata": {},
   "source": [
    "## Create a dataset with our derived variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2892195-d70e-4f64-b730-a74a28941052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "def build_chla_profile_dataset(CHLA: xr.DataArray) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Given CHLA(time, z, lat, lon), compute derived metrics and\n",
    "    return an xr.Dataset suitable for writing to Zarr/NetCDF.\n",
    "    \"\"\"\n",
    "\n",
    "    # Start from CHLA's own dataset so its coords (including z_start/z_end) win\n",
    "    ds = CHLA.to_dataset(name=\"CHLA\")\n",
    "\n",
    "    # ---- Layer thickness (z dimension) ----\n",
    "    z_start = CHLA.coords.get(\"z_start\", None)\n",
    "    z_end   = CHLA.coords.get(\"z_end\", None)\n",
    "\n",
    "    if (z_start is not None) and (z_end is not None):\n",
    "        z_thick = (z_end - z_start).rename(\"z_thickness\")   # (z)\n",
    "    else:\n",
    "        # fallback: uniform layer thickness, e.g. 10 m\n",
    "        z_thick = xr.full_like(CHLA[\"z\"], 10.0).rename(\"z_thickness\")\n",
    "\n",
    "    z_center = CHLA[\"z\"]\n",
    "\n",
    "    # total CHLA in column (used for validity + center-of-mass)\n",
    "    col_total = CHLA.sum(\"z\")          # (time, lat, lon)\n",
    "    valid = col_total > 0              # True where there is some CHLA\n",
    "\n",
    "    # ---- Integrated CHLA (nominal 0–200 m; actual range = z extent) ----\n",
    "    CHLA_int = (CHLA * z_thick).sum(\"z\")\n",
    "    CHLA_int = CHLA_int.where(valid)\n",
    "    CHLA_int.name = \"CHLA_int_0_200\"\n",
    "\n",
    "    # ---- Peak value and depth (NaN-safe) ----\n",
    "    CHLA_filled = CHLA.fillna(-np.inf)\n",
    "    peak_idx = CHLA_filled.argmax(\"z\")       # (time, lat, lon) integer indices\n",
    "\n",
    "    CHLA_peak = CHLA.isel(z=peak_idx).where(valid)\n",
    "    CHLA_peak.name = \"CHLA_peak\"\n",
    "\n",
    "    CHLA_peak_depth = z_center.isel(z=peak_idx).where(valid)\n",
    "    CHLA_peak_depth.name = \"CHLA_peak_depth\"\n",
    "\n",
    "    # ---- Depth-weighted mean depth (center of mass) ----\n",
    "    num = (CHLA * z_center).sum(\"z\")\n",
    "    den = col_total\n",
    "    depth_cm = (num / den).where(valid)\n",
    "    depth_cm.name = \"CHLA_depth_center_of_mass\"\n",
    "\n",
    "    # ---- Attach derived fields to the dataset ----\n",
    "    ds[\"CHLA_int_0_200\"] = CHLA_int\n",
    "    ds[\"CHLA_peak\"] = CHLA_peak\n",
    "    ds[\"CHLA_peak_depth\"] = CHLA_peak_depth\n",
    "    ds[\"CHLA_depth_center_of_mass\"] = depth_cm\n",
    "    ds[\"z_thickness\"] = z_thick\n",
    "\n",
    "    # ---- Variable attributes ----\n",
    "    # CHLA itself should already have attrs from the prediction step\n",
    "    ds[\"CHLA\"].attrs.setdefault(\"units\", \"mg m-3\")\n",
    "    ds[\"CHLA\"].attrs.setdefault(\"long_name\", \"Chlorophyll-a concentration\")\n",
    "    ds[\"CHLA\"].attrs.setdefault(\n",
    "        \"description\",\n",
    "        \"BRT-derived chlorophyll-a profiles from PACE hyperspectral Rrs\",\n",
    "    )\n",
    "\n",
    "    ds[\"CHLA_int_0_200\"].attrs.update(\n",
    "        units=\"mg m-2\",\n",
    "        long_name=\"Depth-integrated chlorophyll-a\",\n",
    "        description=(\n",
    "            \"Vertical integral of CHLA over the available depth bins \"\n",
    "            \"(nominally 0–200 m; actual range defined by z_start/z_end).\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    ds[\"CHLA_peak\"].attrs.update(\n",
    "        units=\"mg m-3\",\n",
    "        long_name=\"Peak chlorophyll-a concentration in the water column\",\n",
    "        description=\"Maximum CHLA value over depth at each (time, lat, lon).\",\n",
    "    )\n",
    "\n",
    "    ds[\"CHLA_peak_depth\"].attrs.update(\n",
    "        units=\"m\",\n",
    "        long_name=\"Depth of peak chlorophyll-a\",\n",
    "        positive=\"down\",\n",
    "        description=(\n",
    "            \"Depth (bin center) where CHLA is maximal in the water column \"\n",
    "            \"at each (time, lat, lon).\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    ds[\"CHLA_depth_center_of_mass\"].attrs.update(\n",
    "        units=\"m\",\n",
    "        long_name=\"Chlorophyll-a depth center of mass\",\n",
    "        positive=\"down\",\n",
    "        description=(\n",
    "            \"Depth of the chlorophyll-a center of mass, computed as \"\n",
    "            \"sum_z(CHLA * z) / sum_z(CHLA).\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    ds[\"z_thickness\"].attrs.update(\n",
    "        units=\"m\",\n",
    "        long_name=\"Layer thickness\",\n",
    "        description=(\n",
    "            \"Thickness of each vertical bin used for depth integration. \"\n",
    "            \"Derived from z_end - z_start when available; otherwise set to a \"\n",
    "            \"uniform nominal thickness.\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # You can still add global ds.attrs later in your pipeline\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3155ea6-e5cf-47fd-af42-83c48d8f6664",
   "metadata": {},
   "source": [
    "## Test pipeline bits\n",
    "\n",
    "Get data and make prediction, build dataset. Save a test file to gcs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5cd45edc-f94f-4c75-b44b-64cfd7616640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 0 of 480\n",
      "Starting 100 of 480\n",
      "Starting 200 of 480\n",
      "Starting 300 of 480\n",
      "Starting 400 of 480\n",
      "Starting wrapping\n",
      "Adding coords\n"
     ]
    }
   ],
   "source": [
    "# Get some Rrs data\n",
    "import earthaccess\n",
    "import xarray as xr\n",
    "\n",
    "day = \"2024-07-08\"\n",
    "\n",
    "auth = earthaccess.login()\n",
    "# are we authenticated?\n",
    "if not auth.authenticated:\n",
    "    # ask for credentials and persist them in a .netrc file\n",
    "    auth.login(strategy=\"interactive\", persist=True)\n",
    "\n",
    "# Get Rrs\n",
    "rrs_results = earthaccess.search_data(\n",
    "    short_name = \"PACE_OCI_L3M_RRS\",\n",
    "    temporal = (day, day),\n",
    "    granule_name=\"*.DAY.*.4km.nc\"\n",
    ")\n",
    "f = earthaccess.open(rrs_results[0:1], pqdm_kwargs={\"disable\": True})\n",
    "rrs_ds = xr.open_dataset(f[0])\n",
    "\n",
    "lat_min, lat_max = 20, 40\n",
    "lon_min, lon_max = -70, -60\n",
    "# Get Rrs for that box\n",
    "R = rrs_ds[\"Rrs\"].sel(\n",
    "    lat=slice(lat_max, lat_min),   # decreasing lat coord: max→min\n",
    "    lon=slice(lon_min, lon_max)\n",
    ")\n",
    "pred = bundle.predict(\n",
    "      R,                  # xr.DataArray/xr.Dataset with lat/lon + predictors\n",
    "      brt_models=bundle.model,    # dict of models by depth bin\n",
    "      feature_cols=bundle.meta['feature_cols'],\n",
    "      consts={'solar_hour': 0, 'type': 1},\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952cfbc1-f1e7-4776-b88e-479e2b3ab17e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_ds = build_chla_profile_dataset(pred)\n",
    "test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "cb252892-1a4e-42f2-b863-e3983c60dc89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded test.pdf → gs://nmfs_odp_nwfsc/CB/fish-pace-datasets/chla-z/netcdf/test.pdf\n"
     ]
    }
   ],
   "source": [
    "# Test saving file to the bucket\n",
    "# stop annoying warnings\n",
    "# https://console.cloud.google.com/storage/browser/nmfs_odp_nwfsc/CB/fish-pace-datasets\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Your application has authenticated using end user credentials\")\n",
    "\n",
    "from google.cloud import storage\n",
    "from pathlib import Path\n",
    "\n",
    "# === Set these ===\n",
    "bucket_name = \"nmfs_odp_nwfsc\"\n",
    "\n",
    "# Create client and bucket\n",
    "client = storage.Client(project=\"noaa-gcs-public-data\")\n",
    "bucket = client.bucket(bucket_name)\n",
    "\n",
    "# Set the file you want to test with\n",
    "test_file = Path(\"test.pdf\")  # change this if using a different file\n",
    "destination_prefix = \"CB/fish-pace-datasets/chla-z/netcdf\"\n",
    "\n",
    "# Create blob and upload\n",
    "blob_path = f\"{destination_prefix}/{test_file.name}\"\n",
    "blob = bucket.blob(blob_path)\n",
    "blob.upload_from_filename(str(test_file))\n",
    "\n",
    "print(f\"Uploaded {test_file.name} → gs://{bucket_name}/{blob_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e6ef4f-4d93-426d-b8a4-473fb5df1ff6",
   "metadata": {},
   "source": [
    "## Full pipeline\n",
    "\n",
    "* dataset to netcdf in google bucket\n",
    "* Zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46416e46-1982-4ec4-962e-24d95b8a3892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "560"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Rrs data\n",
    "import earthaccess\n",
    "import xarray as xr\n",
    "\n",
    "auth = earthaccess.login()\n",
    "# are we authenticated?\n",
    "if not auth.authenticated:\n",
    "    # ask for credentials and persist them in a .netrc file\n",
    "    auth.login(strategy=\"interactive\", persist=True)\n",
    "\n",
    "# Get Rrs\n",
    "rrs_results = earthaccess.search_data(\n",
    "    short_name = \"PACE_OCI_L3M_RRS\",\n",
    "    granule_name=\"*.DAY.*.4km.nc\"\n",
    ")\n",
    "len(rrs_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e589d0e3-e16d-4dbc-b5b0-6bf4661557ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/google/auth/_default.py:108: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 20240305...\n",
      "Starting 0 of 4320\n",
      "Starting 100 of 4320\n",
      "Starting 200 of 4320\n",
      "Starting 300 of 4320\n",
      "Starting 400 of 4320\n",
      "Starting 500 of 4320\n",
      "Starting 600 of 4320\n",
      "Starting 700 of 4320\n",
      "Starting 800 of 4320\n",
      "Starting 900 of 4320\n",
      "Starting 1000 of 4320\n",
      "Starting 1100 of 4320\n",
      "Starting 1200 of 4320\n",
      "Starting 1300 of 4320\n",
      "Starting 1400 of 4320\n",
      "Starting 1500 of 4320\n",
      "Starting 1600 of 4320\n",
      "Starting 1700 of 4320\n",
      "Starting 1800 of 4320\n",
      "Starting 1900 of 4320\n",
      "Starting 2000 of 4320\n",
      "Starting 2100 of 4320\n",
      "Starting 2200 of 4320\n",
      "Starting 2300 of 4320\n",
      "Starting 2400 of 4320\n",
      "Starting 2500 of 4320\n",
      "Starting 2600 of 4320\n",
      "Starting 2700 of 4320\n",
      "Starting 2800 of 4320\n",
      "Starting 2900 of 4320\n",
      "Starting 3000 of 4320\n",
      "Starting 3100 of 4320\n",
      "Starting 3200 of 4320\n",
      "Starting 3300 of 4320\n",
      "Starting 3400 of 4320\n",
      "Starting 3500 of 4320\n",
      "Starting 3600 of 4320\n",
      "Starting 3700 of 4320\n",
      "Starting 3800 of 4320\n",
      "Starting 3900 of 4320\n",
      "Starting 4000 of 4320\n",
      "Starting 4100 of 4320\n",
      "Starting 4200 of 4320\n",
      "Starting 4300 of 4320\n",
      "Starting wrapping\n",
      "Adding coords\n",
      "Adding time\n",
      "CPU times: user 28min 56s, sys: 4min 13s, total: 33min 10s\n",
      "Wall time: 30min 9s\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ds_day' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m<timed exec>:74\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'build_chla_profile_dataset' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtime\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mimport xarray as xr\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mimport numpy as np\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mimport pandas as pd\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mfrom pathlib import Path\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mimport earthaccess\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mfrom google.cloud import storage\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mimport tempfile\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m# Get Rrs data\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mimport earthaccess\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mimport xarray as xr\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mauth = earthaccess.login()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m# are we authenticated?\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mif not auth.authenticated:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    # ask for credentials and persist them in a .netrc file\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    auth.login(strategy=\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minteractive\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m, persist=True)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m# Get Rrs\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mrrs_results = earthaccess.search_data(\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    short_name = \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPACE_OCI_L3M_RRS\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    granule_name=\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m*.DAY.*.4km.nc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m# === Set these ===\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mbucket_name = \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnmfs_odp_nwfsc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m# Create client and bucket\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mclient = storage.Client(project=\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnoaa-gcs-public-data\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mbucket = client.bucket(bucket_name)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mdestination_prefix = \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mCB/fish-pace-datasets/chla-z/netcdf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mlat_chunk = 100\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mlon_chunk = 100\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m# just test on first two granules\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mfor res in rrs_results[0:1]:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    # day as ISO string from UMM\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    day_iso = res[\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mumm\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m][\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTemporalExtent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m][\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mRangeDateTime\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m][\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mBeginningDateTime\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    day = pd.to_datetime(day_iso)          # Timestamp\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    day_str = day.strftime(\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m%\u001b[39;49m\u001b[33;43mY\u001b[39;49m\u001b[33;43m%\u001b[39;49m\u001b[33;43mm\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    print(f\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mProcessing \u001b[39;49m\u001b[38;5;132;43;01m{day_str}\u001b[39;49;00m\u001b[33;43m...\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    # auth per granule (you can optimize this later if you want)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    auth = earthaccess.login()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    files = earthaccess.open([res], pqdm_kwargs=\u001b[39;49m\u001b[33;43m{\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdisable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m: True})\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    # open dataset\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    rrs_ds = xr.open_dataset(files[0])\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    # debugging line\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    #rrs_ds = rrs_ds.sel(lat=slice(40, 20), lon=slice(-70, -60) )\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    try:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        # Rrs for that day\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        # (if the file has only one time, squeeze; otherwise select)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        if \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtime\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m in rrs_ds.dims:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m            R = rrs_ds[\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mRrs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m].sel(time=day).squeeze(\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtime\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        else:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m            R = rrs_ds[\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mRrs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        R = R.transpose(\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlat\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m, \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlon\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m, \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwavelength\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        # BRT predictions for all depths\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        pred = bundle.predict(\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m            R,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m            brt_models=bundle.model,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m            feature_cols=bundle.meta[\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfeature_cols\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m],\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m            consts=\u001b[39;49m\u001b[33;43m{\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msolar_hour\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m: 0, \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtype\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m: 1},\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m            chunk_size_lat=100,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m            time=day.to_datetime64(),   # time coord length 1\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m            z_name=\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mz\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m            silent=False,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        )  # (time=1, z, lat, lon), float32\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        # Build full dataset with integrated/peak metrics\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        ds_day = build_chla_profile_dataset(pred)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        # Add metadata\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        ds_day[\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mCHLA\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m].attrs.update(\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m            units=\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmg m-3\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m            long_name=\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mChlorophyll-a concentration\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m            description=\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mBRT-derived CHLA profiles from PACE hyperspectral Rrs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        )\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        ds_day[\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mz\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m].attrs.update(units=\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mm\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m, long_name=\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdepth (bin center)\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        ds_day[\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlat\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m].attrs.update(units=\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdegrees_north\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        ds_day[\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlon\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m].attrs.update(units=\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdegrees_east\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        ds_day.attrs[\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msource\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m] = \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mBRT model trained on BGC-Argo + OOI matchups\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        ds_day.attrs[\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel_bundle\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m] = Path(\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpath/to/bundle.zip\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m).name\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        # Chunking for NetCDF\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        encoding = \u001b[39;49m\u001b[33;43m{\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m            \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mCHLA\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m: \u001b[39;49m\u001b[33;43m{\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m                \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdtype\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m: \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfloat32\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m                \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mzlib\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m: True,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m                \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcomplevel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m: 4,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m                \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mchunksizes\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m: (1, ds_day.sizes[\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mz\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m], lat_chunk, lon_chunk),\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m            }\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m            # you can add encodings for derived vars later if desired\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        }\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        # 4. Write to a local temporary NetCDF, then upload to GCS\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        tmp_dir = Path(tempfile.gettempdir())\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        local_path = tmp_dir / f\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mchla_z_\u001b[39;49m\u001b[38;5;132;43;01m{day_str}\u001b[39;49;00m\u001b[33;43m.nc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        ds_day.to_netcdf(\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m            local_path,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m            engine=\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mh5netcdf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m            encoding=encoding,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        )\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        blob_path = f\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{destination_prefix}\u001b[39;49;00m\u001b[33;43m/chla_z_\u001b[39;49m\u001b[38;5;132;43;01m{day_str}\u001b[39;49;00m\u001b[33;43m.nc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        blob = bucket.blob(blob_path)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        blob.upload_from_filename(str(local_path))\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        print(f\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWrote \u001b[39;49m\u001b[38;5;132;43;01m{local_path}\u001b[39;49;00m\u001b[33;43m → gs://\u001b[39;49m\u001b[38;5;132;43;01m{bucket_name}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[38;5;132;43;01m{blob_path}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    finally:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        rrs_ds.close()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        del ds_day, pred, R, rrs_ds\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/srv/conda/envs/notebook/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2565\u001b[39m, in \u001b[36mInteractiveShell.run_cell_magic\u001b[39m\u001b[34m(self, magic_name, line, cell)\u001b[39m\n\u001b[32m   2563\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.builtin_trap:\n\u001b[32m   2564\u001b[39m     args = (magic_arg_s, cell)\n\u001b[32m-> \u001b[39m\u001b[32m2565\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[32m   2568\u001b[39m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[32m   2569\u001b[39m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[32m   2570\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/srv/conda/envs/notebook/lib/python3.11/site-packages/IPython/core/magics/execution.py:1470\u001b[39m, in \u001b[36mExecutionMagics.time\u001b[39m\u001b[34m(self, line, cell, local_ns)\u001b[39m\n\u001b[32m   1468\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m interrupt_occured:\n\u001b[32m   1469\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exit_on_interrupt \u001b[38;5;129;01mand\u001b[39;00m captured_exception:\n\u001b[32m-> \u001b[39m\u001b[32m1470\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m captured_exception\n\u001b[32m   1471\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   1472\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/srv/conda/envs/notebook/lib/python3.11/site-packages/IPython/core/magics/execution.py:1434\u001b[39m, in \u001b[36mExecutionMagics.time\u001b[39m\u001b[34m(self, line, cell, local_ns)\u001b[39m\n\u001b[32m   1432\u001b[39m st = clock2()\n\u001b[32m   1433\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1434\u001b[39m     \u001b[43mexec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_ns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1435\u001b[39m     out = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1436\u001b[39m     \u001b[38;5;66;03m# multi-line %%time case\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<timed exec>:117\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'ds_day' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import earthaccess\n",
    "from google.cloud import storage\n",
    "import tempfile\n",
    "\n",
    "# Get Rrs data\n",
    "import earthaccess\n",
    "import xarray as xr\n",
    "\n",
    "auth = earthaccess.login()\n",
    "# are we authenticated?\n",
    "if not auth.authenticated:\n",
    "    # ask for credentials and persist them in a .netrc file\n",
    "    auth.login(strategy=\"interactive\", persist=True)\n",
    "\n",
    "# Get Rrs\n",
    "rrs_results = earthaccess.search_data(\n",
    "    short_name = \"PACE_OCI_L3M_RRS\",\n",
    "    granule_name=\"*.DAY.*.4km.nc\"\n",
    ")\n",
    "\n",
    "# === Set these ===\n",
    "bucket_name = \"nmfs_odp_nwfsc\"\n",
    "# Create client and bucket\n",
    "client = storage.Client(project=\"noaa-gcs-public-data\")\n",
    "bucket = client.bucket(bucket_name)\n",
    "destination_prefix = \"CB/fish-pace-datasets/chla-z/netcdf\"\n",
    "\n",
    "lat_chunk = 100\n",
    "lon_chunk = 100\n",
    "\n",
    "# just test on first two granules\n",
    "for res in rrs_results[0:1]:\n",
    "    # day as ISO string from UMM\n",
    "    day_iso = res[\"umm\"][\"TemporalExtent\"][\"RangeDateTime\"][\"BeginningDateTime\"]\n",
    "    day = pd.to_datetime(day_iso)          # Timestamp\n",
    "    day_str = day.strftime(\"%Y%m%d\")\n",
    "    print(f\"Processing {day_str}...\")\n",
    "\n",
    "    # auth per granule (you can optimize this later if you want)\n",
    "    auth = earthaccess.login()\n",
    "    files = earthaccess.open([res], pqdm_kwargs={\"disable\": True})\n",
    "\n",
    "    # open dataset\n",
    "    rrs_ds = xr.open_dataset(files[0])\n",
    "    # debugging line\n",
    "    #rrs_ds = rrs_ds.sel(lat=slice(40, 20), lon=slice(-70, -60) )\n",
    "\n",
    "    try:\n",
    "        # Rrs for that day\n",
    "        # (if the file has only one time, squeeze; otherwise select)\n",
    "        if \"time\" in rrs_ds.dims:\n",
    "            R = rrs_ds[\"Rrs\"].sel(time=day).squeeze(\"time\")\n",
    "        else:\n",
    "            R = rrs_ds[\"Rrs\"]\n",
    "        R = R.transpose(\"lat\", \"lon\", \"wavelength\")\n",
    "\n",
    "        # BRT predictions for all depths\n",
    "        pred = bundle.predict(\n",
    "            R,\n",
    "            brt_models=bundle.model,\n",
    "            feature_cols=bundle.meta[\"feature_cols\"],\n",
    "            consts={\"solar_hour\": 0, \"type\": 1},\n",
    "            chunk_size_lat=100,\n",
    "            time=day.to_datetime64(),   # time coord length 1\n",
    "            z_name=\"z\",\n",
    "            silent=False,\n",
    "        )  # (time=1, z, lat, lon), float32\n",
    "\n",
    "        # Build full dataset with integrated/peak metrics\n",
    "        ds_day = build_chla_profile_dataset(pred)\n",
    "\n",
    "        # Add metadata\n",
    "        ds_day[\"CHLA\"].attrs.update(\n",
    "            units=\"mg m-3\",\n",
    "            long_name=\"Chlorophyll-a concentration\",\n",
    "            description=\"BRT-derived CHLA profiles from PACE hyperspectral Rrs\",\n",
    "        )\n",
    "        ds_day[\"z\"].attrs.update(units=\"m\", long_name=\"depth (bin center)\")\n",
    "        ds_day[\"lat\"].attrs.update(units=\"degrees_north\")\n",
    "        ds_day[\"lon\"].attrs.update(units=\"degrees_east\")\n",
    "        ds_day.attrs[\"source\"] = \"BRT model trained on BGC-Argo + OOI matchups\"\n",
    "        ds_day.attrs[\"model_bundle\"] = Path(\"path/to/bundle.zip\").name\n",
    "\n",
    "        # Chunking for NetCDF\n",
    "        encoding = {\n",
    "            \"CHLA\": {\n",
    "                \"dtype\": \"float32\",\n",
    "                \"zlib\": True,\n",
    "                \"complevel\": 4,\n",
    "                \"chunksizes\": (1, ds_day.sizes[\"z\"], lat_chunk, lon_chunk),\n",
    "            }\n",
    "            # you can add encodings for derived vars later if desired\n",
    "        }\n",
    "\n",
    "        # 4. Write to a local temporary NetCDF, then upload to GCS\n",
    "        tmp_dir = Path(tempfile.gettempdir())\n",
    "        local_path = tmp_dir / f\"chla_z_{day_str}.nc\"\n",
    "        \n",
    "        ds_day.to_netcdf(\n",
    "            local_path,\n",
    "            engine=\"h5netcdf\",\n",
    "            encoding=encoding,\n",
    "        )\n",
    "        \n",
    "        blob_path = f\"{destination_prefix}/chla_z_{day_str}.nc\"\n",
    "        blob = bucket.blob(blob_path)\n",
    "        blob.upload_from_filename(str(local_path))\n",
    "        \n",
    "        print(f\"Wrote {local_path} → gs://{bucket_name}/{blob_path}\")\n",
    "        \n",
    "    finally:\n",
    "        rrs_ds.close()\n",
    "        del ds_day, pred, R, rrs_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2e20d480-137e-455e-9ace-23018f6b5669",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b29ea58-811f-4ce6-b165-426e8de24ac8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
