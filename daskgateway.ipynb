{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5727d3b-fd67-47b9-9ba9-403fb96d8538",
   "metadata": {},
   "source": [
    "# Process daily files in dask gateway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55b17c21-afb1-4c89-b272-cbffc07a75e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'ml_utils' from '/home/jovyan/chla-z-modeling/ml_utils.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Core data handling and plotting libraries ---\n",
    "import earthaccess\n",
    "import xarray as xr       # for working with labeled multi-dimensional arrays\n",
    "import numpy as np        # for numerical operations on arrays\n",
    "import pandas as pd\n",
    "# --- Custom python functions ---\n",
    "import os, importlib\n",
    "# Looks to see if you have the file already and if not, downloads from GitHub\n",
    "if not os.path.exists(\"ml_utils.py\"):\n",
    "    !wget -q https://raw.githubusercontent.com/fish-pace/2025-tutorials/main/ml_utils.py\n",
    "\n",
    "import ml_utils as mu\n",
    "importlib.reload(mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c0d43fb-fb31-48fc-b15f-4e1f565caaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# In reality: read from a secret, NOT hardcoded\n",
    "with open(\"/home/jovyan/.config/gcloud/application_default_credentials.json\") as f:\n",
    "    GCP_SA_JSON = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e1e3d3a-cb19-4201-91b4-98aefc16479b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def process_one_granule(\n",
    "    res,\n",
    "    lat_chunk=LAT_CHUNK,\n",
    "    lon_chunk=LON_CHUNK,\n",
    "    bucket_name=BUCKET_NAME,\n",
    "    destination_prefix=DESTINATION_PREFIX,\n",
    "    force_rerun=FORCE_RERUN,\n",
    "    ed_username=None,\n",
    "    ed_password=None,\n",
    "    gcp_sa_json=None,\n",
    "):\n",
    "    import os\n",
    "    import tempfile\n",
    "    import earthaccess\n",
    "    import xarray as xr\n",
    "    import pandas as pd\n",
    "    from google.cloud import storage\n",
    "    from pathlib import Path\n",
    "\n",
    "    # --- EARTHACCESS AUTH VIA ENV VARS (inside worker) ---\n",
    "    if ed_username is not None and ed_password is not None:\n",
    "        os.environ[\"EARTHDATA_USERNAME\"] = ed_username\n",
    "        os.environ[\"EARTHDATA_PASSWORD\"] = ed_password\n",
    "\n",
    "    auth = earthaccess.login(strategy=\"environment\", persist=False)\n",
    "\n",
    "    # --- GCP AUTH VIA JSON TEXT (inside worker) ---\n",
    "    cred_path = None\n",
    "    if gcp_sa_json is not None:\n",
    "        tmp_dir = tempfile.gettempdir()\n",
    "        # unique-ish filename per worker/process\n",
    "        cred_path = os.path.join(tmp_dir, \"gcp_sa_worker.json\")\n",
    "        with open(cred_path, \"w\") as f:\n",
    "            f.write(gcp_sa_json)\n",
    "        os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = cred_path\n",
    "\n",
    "    # -------------------------------\n",
    "    #  Normal per-day pipeline below\n",
    "    # -------------------------------\n",
    "    day_iso = res[\"umm\"][\"TemporalExtent\"][\"RangeDateTime\"][\"BeginningDateTime\"]\n",
    "    day = pd.to_datetime(day_iso)\n",
    "    day_str = day.strftime(\"%Y%m%d\")\n",
    "\n",
    "    storage_client = storage.Client(project=\"noaa-gcs-public-data\")\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob_path = f\"{destination_prefix}/chla_z_{day_str}.nc\"\n",
    "    blob = bucket.blob(blob_path)\n",
    "\n",
    "    if blob.exists() and not force_rerun:\n",
    "        msg = f\"[{day_str}] SKIP (exists at gs://{bucket_name}/{blob_path})\"\n",
    "        print(msg)\n",
    "        return msg\n",
    "\n",
    "    files = earthaccess.open([res], auth=auth, pqdm_kwargs={\"disable\": True})\n",
    "    rrs_ds = xr.open_dataset(files[0])\n",
    "\n",
    "    try:\n",
    "        if \"time\" in rrs_ds.dims:\n",
    "            R = rrs_ds[\"Rrs\"].sel(time=day).squeeze(\"time\")\n",
    "        else:\n",
    "            R = rrs_ds[\"Rrs\"]\n",
    "        R = R.transpose(\"lat\", \"lon\", \"wavelength\")\n",
    "\n",
    "        pred = bundle.predict(\n",
    "            R,\n",
    "            brt_models=bundle.model,\n",
    "            feature_cols=bundle.meta[\"feature_cols\"],\n",
    "            consts={\"solar_hour\": 0, \"type\": 1},\n",
    "            chunk_size_lat=100,\n",
    "            time=day.to_datetime64(),\n",
    "            z_name=\"z\",\n",
    "            silent=True,\n",
    "        )\n",
    "\n",
    "        ds_day = build_chla_profile_dataset(pred)\n",
    "\n",
    "        tmp_dir = Path(tempfile.gettempdir())\n",
    "        local_path = tmp_dir / f\"chla_z_{day_str}.nc\"\n",
    "\n",
    "        encoding = {\n",
    "            \"CHLA\": {\n",
    "                \"dtype\": \"float32\",\n",
    "                \"zlib\": True,\n",
    "                \"complevel\": 4,\n",
    "                \"chunksizes\": (1, ds_day.sizes[\"z\"], lat_chunk, lon_chunk),\n",
    "            }\n",
    "        }\n",
    "\n",
    "        ds_day.to_netcdf(local_path, engine=\"h5netcdf\", encoding=encoding)\n",
    "        blob.upload_from_filename(str(local_path))\n",
    "        local_path.unlink(missing_ok=True)\n",
    "\n",
    "        gcs_url = f\"gs://{bucket_name}/{blob_path}\"\n",
    "        msg = f\"[{day_str}] WROTE {gcs_url}\"\n",
    "        print(msg)\n",
    "        return msg\n",
    "\n",
    "    finally:\n",
    "        rrs_ds.close()\n",
    "        # optional: clean up the creds file\n",
    "        if cred_path is not None:\n",
    "            try:\n",
    "                os.remove(cred_path)\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7487b5f4-8e86-4a77-9de7-f8385a123e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded ML bundle from: models/brt_chla_profiles_bundle.zip\n",
      "  model_kind : pickle\n",
      "  model_type : collection (dict), n_submodels=20\n",
      "  example key: CHLA_0_10\n",
      "  target     : log10_CHLA_A_B depth bins\n",
      "  features   : 174 columns\n",
      "  train/test : 4408 / 1102 rows\n",
      "  dataset    : 5510 rows stored in bundle\n",
      "\n",
      "Usage example (Python):\n",
      "  bundle = load_ml_bundle('path/to/bundle.zip')\n",
      "  # Predict using helper 'predict_all_depths_for_day'\n",
      "  # Example: predict all depths for one day from a BRF dataset R\n",
      "  pred = bundle.predict(\n",
      "      R_dataset,                  # xr.DataArray/xr.Dataset with lat/lon + predictors\n",
      "      brt_models=bundle.model,    # dict of models by depth bin\n",
      "      feature_cols=bundle.meta['feature_cols'],\n",
      "      consts={'solar_hour': 12.0, 'type': 1},\n",
      "  )  # -> e.g. CHLA(time?, z, lat, lon)\n",
      "\n",
      "  # Plot using helper 'make_plot_pred_map'\n",
      "  fig, ax = bundle.plot(pred_da, pred_label='Prediction')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in model\n",
    "\n",
    "## Load model\n",
    "bundle = mu.load_ml_bundle(\"models/brt_chla_profiles_bundle.zip\")\n",
    "brt_models = bundle.model\n",
    "meta = bundle.meta\n",
    "rrs_cols = meta[\"rrs_cols\"]\n",
    "chl_cols = meta[\"y_col\"]\n",
    "extra = meta[\"extra_cols\"]\n",
    "dataset = bundle.data[\"dataset\"]\n",
    "train_idx = bundle.data[\"train_idx\"]\n",
    "test_idx = bundle.data[\"test_idx\"]\n",
    "X_train = bundle.data[\"X_train\"]\n",
    "X_test = bundle.data[\"X_test\"]\n",
    "y_train_all = bundle.data[\"y_train\"]\n",
    "y_test_all = bundle.data[\"y_test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499ec293-cd7a-4e9e-8468-ef5e08318e90",
   "metadata": {},
   "source": [
    "# Set up Dask Gateway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f6997c-bd0b-489b-a9dc-56778ede8b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_gateway import Gateway\n",
    "\n",
    "gateway = Gateway()\n",
    "options = gateway.cluster_options()\n",
    "\n",
    "# I don't know how to decide. I know that one day takes 5Gb RAM\n",
    "# options.worker_cores = 4\n",
    "# options.worker_memory = \"32GiB\"\n",
    "\n",
    "cluster = gateway.new_cluster(options)\n",
    "\n",
    "# I don't know how to decide\n",
    "# cluster.scale(8)  # say 8 workers\n",
    "\n",
    "# I have 560 days and each day is 30min. I don't want this to take all day\n",
    "cluster.adapt(minimum=4, maximum=16)\n",
    "\n",
    "from dask.distributed import Client\n",
    "client = cluster.get_client()\n",
    "print(cluster)\n",
    "print(client)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88037da5-0885-4406-8756-508c70f84027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we already have rrs_results from earthaccess.search_data\n",
    "# maybe subset by date or just take all DAY granules\n",
    "granules = rrs_results[:10]   # or rrs_results[:100] for testing\n",
    "\n",
    "# one Dask task per granule\n",
    "futures = client.map(process_one_granule, granules)\n",
    "\n",
    "# block until all are done, get the GCS URLs (or errors)\n",
    "results = client.gather(futures)\n",
    "print(\"Uploaded daily files:\")\n",
    "for r in results:\n",
    "    print(\"  \", r)\n",
    "\n",
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0657a680-97a5-4377-a255-4ce9b88cf57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import os\n",
    "import netrc\n",
    "import json\n",
    "\n",
    "netrc_path = os.path.expanduser(\"~/.netrc\")\n",
    "auth = netrc.netrc(netrc_path)\n",
    "login, account, password = auth.authenticators(\"urs.earthdata.nasa.gov\")\n",
    "ED_USER = login\n",
    "ED_PASS = password\n",
    "with open(\"/home/jovyan/.config/gcloud/application_default_credentials.json\") as f:\n",
    "    GCP_SA_JSON = f.read()\n",
    "    \n",
    "fn = partial(\n",
    "    process_one_granule,\n",
    "    ed_username=ED_USER,\n",
    "    ed_password=ED_PASS,\n",
    "    gcp_sa_json=GCP_SA_JSON,\n",
    ")\n",
    "\n",
    "futures = client.map(fn, rrs_results_subset)\n",
    "results = client.gather(futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04122a52-247f-4774-b028-fc91ad38f089",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import netrc\n",
    "\n",
    "netrc_path = os.path.expanduser(\"~/.netrc\")\n",
    "auth = netrc.netrc(netrc_path)\n",
    "login, account, password = auth.authenticators(\"urs.earthdata.nasa.gov\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8dff6957-5b60-49d6-b22d-af59b38e6cd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C6m3U3iYTTQo'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3ef623-6142-453d-98b8-272f05b1c487",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
